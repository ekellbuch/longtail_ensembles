Comment: >
  
  # debug
  python scripts/run.py --config-name="run_gpu_cifar10lt" trainer_cfg.fast_dev_run=1 eval_cfg.random_eval=1 data_cfg.seed=0

project_name: longtail_gpu_cifar10lt

# Data config:

data_cfg:
  test_set: "IMBALANCECIFAR10Aug_v2"    # pytorch lightning datamodule
  data_dir: ${oc.env:HOME}/pytorch_datasets # location of test set
  ood_dataset: cifar10_1  # ood dataset
  corruption: null  # -c corruption_type in ood_dataset
  level: null  # -c corruption_level in ood_dataset
  version: v4  # cifar10_1 version v4 or v6
  num_classes: 10   # number of classes in dataset
  samples_per_class: null   # number of samples per class
  batch_size: 256
  num_workers: 32  # dataloader number of workers to use
  seed: null      # dataloader seed
  valid_size: 0.1  # assign val_set 0.1*train_set size
  imb_type: exp   # params for imbalanced cifar10/0
  imb_factor: 0.01  # params for imbalanced cifar10/0


fit_cfg:
  ckpt_path: null

trainer_cfg:
  fast_dev_run: 0
  logger: "wandb"
  deterministic: false
  log_every_n_steps: 50
  max_epochs: 200
  precision: 32
  devices: auto
  accelerator: auto
  val_check_interval: 1.0  # set to 1 for early stopping
  plugins: null

# training flags
eval_cfg:
  test_phase: 0   # bool if we are on test phase, do not train
  random_eval: 0  # random eval do not store logits.
  store_train: 0  # store logits on train set
  return_model: 0  # return model trainer for debugging

seed: null

module_cfg:
  module: base  # pytorch lightning model module
  classifier: resnet32_cfa   # name of network
  pretrained: 0
  pretrained-path: null
  learning_rate: 0.05
  weight_decay: 0.0005
  lamb: 0.5
  scheduler: step_ldam
  temperature_scaling: 0
  checkpoint: null
  samples_per_class: null


callbacks:
  gradnorm: 0
  checkpoint_callback: 1
  early_stopping: 0
  lr_monitor: 1

early_stop_cfg:
  monitor: loss/val
  mode: min
  verbose: true
  min_delta: 0.0001
  patience: 10  # related to val_check_interval


# Temperature scaling parameters
ts_cfg:
  use_train_loss: false  # if False, uses cross entropy loss
  max_epochs: 1
  init_temp: 1.5  # init temperature
  ts_module: "ts_base" # ts and ts_vector
  opt_params:
    lr: 0.01
    max_iter: 50
    line_search_fn: "strong_wolfe"
  trainer_args:
    gradient_clip_val: 0.5
    gradient_clip_algorithm: "value"


hydra:
  run:
    dir: ./outputs/${now:%y-%m-%d}/${now:%H-%M-%S}